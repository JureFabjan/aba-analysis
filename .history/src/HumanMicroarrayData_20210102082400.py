from allensdk.api.queries.rma_api import RmaApi
from allensdk.api.cache import Cache

from types import SimpleNamespace

import numpy as np
import pandas as pd
import glob


import Utils
import Constants

from StructureMap import StructureMap

class HumanMicroarrayData:

    def __init__(self, geneAcronym):
        self.geneAcronym = geneAcronym
        self.cache_path = Constants.DATAFRAME_CACHE + f'human\\{geneAcronym}\\'
    
    # ok, now we got n probes with m expression-levels & z-scores
    # we also got m samples that describe which donor and which structure each expression-level stems from
    # we have to be aware that the expression-levels are retrieved from a probe, which represents a plane through the brain.
    # so if the plane of the probe is not cutting through a specific brain-region, then there are null-values present for the expression-level.
    # details: http://help.brain-map.org/display/humanbrain/API

    # TODO: this is quite inefficient code. let's review it later
    def transformExpressionData(self, expressionData):

        # this class allows us to add attributes to an object
        # https://docs.python.org/3/library/types.html#types.SimpleNamespace
        combined = SimpleNamespace()

        setattr(combined, 'samples', []) 
        setattr(combined, 'expression_levels', [])
        setattr(combined, 'z_scores', [])

        for probe in expressionData["probes"]:
            # https://stackoverflow.com/questions/30522724/take-multiple-lists-into-dataframe
            combined.samples += expressionData["samples"]
            combined.expression_levels += probe["expression_level"]
            combined.z_scores += probe["z-score"] # ! z-scores are NOT comparable with ISH-data for mice. these z-scores are only intended for isolated analysis!

        # https://stackoverflow.com/questions/29325458/dictionary-column-in-pandas-dataframe
        data = pd.DataFrame({"expression_level": combined.expression_levels, "z-score": combined.z_scores},
                                    dtype=np.float32) # setting this type is important for later aggregation. else, pandas throws an error for mean & var

        def unpack_dict_list(dict_list, attribute, prefix):
            return pd.DataFrame.from_dict([dict_list[i][attribute] for i in range(len(dict_list))]).add_prefix(prefix) # prefix to prevent naming conflicts

        # attributes with their respective prefix to prevent ambiguous column-names
        attributes = [("donor", ""), ("sample", "sample_"), ("structure", "structure_")] 

        # note that here, the * is the splat-operator
        data = pd.concat([*[unpack_dict_list(combined.samples, attr[0], attr[1]) for attr in attributes], data], axis=1)

        # the z-scores provided here come with some side-notes, according to http://help.brain-map.org/display/humanbrain/API 
        # "Note: z-score is computed independently for each probe over all donors and samples."
        # ! But we don't have probes in ISH data for mice, so we would not be able to compare these numbers.
        # ! To get comparable numbers, we calculate a global z-score.
        # ! As there is only one donor per brain-region, we can simply use the expression_levels:
        data["global-z-score"] = Utils.z_score(data.expression_level)

        # dropna is super slow, so we use this approach instead:
        data = data[data['expression_level'].notnull() & data['z-score'].notnull() & data['global-z-score'].notnull()]

        return data 

    def get(self, from_cache, aggregations): # load data once with use_cache = True, then change it to False to read it from disk instead of fetching it from the api
        if not from_cache:
          # we use the RmaApi to query specific information, such as the section data sets of a specific gene
          # for docs, see: https://alleninstitute.github.io/AllenSDK/allensdk.api.queries.rma_api.html
          rma = RmaApi() 

          # the cache_writeer allows us to easily cache the results
          cache_writer = Cache()

          # ok, so we don't need to do multiple requests to forward data from a model to a service, but simply use the pipe-concept:
          # http://help.brain-map.org/display/api/Service+Pipelines
          # e.g. this finds all probes for gabra4 and then queries the microarray-expression data for these probes. note that variables generated by a pipe are referenced by $variableName

          query = ("http://api.brain-map.org/api/v2/data/query.json?criteria="
                  f"model::Probe,rma::criteria,gene[acronym$il{self.geneAcronym}],rma::options[num_rows$eqall],"
                  "pipe::list[probes$eq'id'],"
                  "service::human_microarray_expression[probes$eq$probes]")

          data = cache_writer.wrap(
                  rma.json_msg_query,
                  path=Utils.makedir(f'cache\\human_microarray-expr\\{self.geneAcronym}') + '\\cache.json',
                  cache=not from_cache, # the semantics of this function are a bit weird. providing True means: add it to the cache,
                  url=query
              )
          
          structure_map = StructureMap(reference_space_key = 'annotation/ccf_2017', resolution = 25).get(structure_graph_id=10) # , annotation, meta 
          
          data = self.transformExpressionData(data)

          # https://stackoverflow.com/questions/19125091/pandas-merge-how-to-avoid-duplicating-columns
          # to avoid automatic renaming the duplicate columns by removing any duplicate-column
          # note that our merge-condition is index vs structure_id. because structure_id is the index of structure_map, 
          # it is not identified as a duplicate column.
          data = data[data.columns.difference(structure_map.columns)]

          ret = Utils.merge_with_structure(data, structure_map, Utils.VALUE_COLUMNS, aggregations)
          
          Utils.save(ret, self.cache_path, 'cache.pkl')

          return { 'data': ret, 'name': 'human' }

        else:
          if not glob.glob(self.cache_path):
            #raise Exception(f"No cached dataframe found. Check whether you have access to file '{self.cache_path}' and whether it exists.")
            Utils.log.warning(f"No cached dataframe found. Check whether you have access to file '{self.cache_path}' and whether it exists.")
            return self.get(False, aggregations)
          return { 'data': Utils.load(self.cache_path + 'cache.pkl'), 'name': 'human' }
